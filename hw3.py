# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-JJRYFK46dUdagOAcu8haxlLxLy4SQP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import cv2
import statistics as sts
import warnings
warnings.filterwarnings('ignore')

from time import time

train_df = pd.read_csv('/content/drive/MyDrive/Homework3/Q1_Train_Data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Homework3/Q1_Test_Data.csv')
validation_df = pd.read_csv('/content/drive/MyDrive/Homework3/Q1_Validation_Data.csv')

emotions = {'angry':0, 'disgust':1, 'fear':2, 'happy':3, 
            'sad':4, 'surprise':5, 'neutral':6}

train_angry_df = train_df.loc[train_df['emotion'] == 0]
train_angry_df = train_angry_df.reset_index(drop = True)

train_disgust_df = train_df.loc[train_df['emotion'] == 1]
train_disgust_df = train_disgust_df.reset_index(drop = True)

train_fear_df = train_df.loc[train_df['emotion'] == 2]
train_fear_df = train_fear_df.reset_index(drop = True)

train_happy_df = train_df.loc[train_df['emotion'] == 3]
train_happy_df = train_happy_df.reset_index(drop = True)

train_sad_df = train_df.loc[train_df['emotion'] == 4]
train_sad_df = train_sad_df.reset_index(drop = True)

train_surprise_df = train_df.loc[train_df['emotion'] == 5]
train_surprise_df = train_surprise_df.reset_index(drop = True)

train_neutral_df = train_df.loc[train_df['emotion'] == 6]
train_neutral_df = train_neutral_df.reset_index(drop = True)

images = [train_angry_df['pixels'][0], train_angry_df['pixels'][1], 
          train_disgust_df['pixels'][0], train_disgust_df['pixels'][1], 
          train_fear_df['pixels'][0], train_fear_df['pixels'][1], 
          train_happy_df['pixels'][0], train_happy_df['pixels'][1], 
          train_sad_df['pixels'][0], train_sad_df['pixels'][1], 
          train_surprise_df['pixels'][0], train_surprise_df['pixels'][1], 
          train_neutral_df['pixels'][0], train_neutral_df['pixels'][1]]

def process_image(image_str):
  img_flat = image_str.split()
  img_flat = list(map(lambda x: float(x), img_flat))

  mean = sts.mean(img_flat)
  stdev = sts.stdev(img_flat)

  img_arr = np.asarray(img_flat)
  img_arr = (img_arr - mean)/ max(stdev, 1/np.sqrt(48*48))
  img = img_arr.reshape(48, 48)

  return img

# reading images
images = [process_image(image) for image in images]

fig = plt.figure(figsize=(6, 24))

# setting values to rows and column variables
rows = 7
columns = 2

for i, image in enumerate(images):
  fig.add_subplot(rows, columns, i+1)
  plt.imshow(image, cmap = 'gray', aspect = 'auto')
  plt.axis('off')
  if i%2 == 0:
    emotion = list(emotions.keys())[i//2]
  plt.title('{}'.format(emotion))

"""**a.** Two images per class is displayed above."""

print("Number of samples for angry emotion = {}". format(len(train_angry_df)))
print("Number of samples for disgust emotion = {}". format(len(train_disgust_df)))
print("Number of samples for fear emotion = {}". format(len(train_fear_df)))
print("Number of samples for happy emotion = {}". format(len(train_happy_df)))
print("Number of samples for sad emotion = {}". format(len(train_sad_df)))
print("Number of samples for surprise emotion = {}". format(len(train_surprise_df)))
print("Number of samples for neutral emotion = {}". format(len(train_neutral_df)))

"""**b.** number of samples per emotion is listed below

*   Angry: 3995
*   Disgust: 436
*   Fear: 4097
*   Happy: 7215
*   Sad: 4830
*   Surprise: 3171
*   Neutral: 4965
"""

X_train = np.asarray([process_image(X) for X in list(train_df['pixels'])])
X_train.shape

y_train = [tf.keras.utils.to_categorical(y, num_classes = 7, dtype = "float32") for y in list(train_df['emotion'])]
y_train = np.asarray(y_train)
y_train.shape

X_val =  np.asarray([process_image(X) for X in list(validation_df['pixels'])])
X_val.shape

y_val = [tf.keras.utils.to_categorical(y, num_classes = 7, dtype = "float32") for y in list(validation_df['emotion'])]
y_val = np.asarray(y_val)
y_val.shape

X_test =  np.asarray([process_image(X) for X in list(test_df['pixels'])])
X_test.shape

y_test = [tf.keras.utils.to_categorical(y, num_classes = 7, dtype = "float32") for y in list(test_df['emotion'])]
y_test = np.asarray(y_test)
y_test.shape

X_train_fnn = X_train.reshape(-1, 2304)
X_val_fnn = X_val.reshape(-1, 2304)
X_test_fnn = X_test.reshape(-1, 2304)

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint
from keras import regularizers

# Neural network
fnn_model1 = Sequential()
fnn_model1.add(Dense(128, input_dim = 2304, kernel_regularizer = regularizers.l2(l2=0.01), activation = 'relu'))
fnn_model1.add(Dense(64, kernel_regularizer = regularizers.l2(l2=0.01), activation= 'relu'))
fnn_model1.add(Dense(32, kernel_regularizer = regularizers.l2(l2=0.01), activation = 'relu'))
fnn_model1.add(Dense(7, activation = 'softmax'))

fnn_checkpoint1 = ModelCheckpoint("best_fnn_model1.hdf5", monitor = 'val_loss', verbose = 1, 
                                  save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)

fnn_model1.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_fnn1 = fnn_model1.fit(X_train_fnn, y_train, 
                    validation_data = (X_val_fnn, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[fnn_checkpoint1])

time_to_run = time() - start

fnn_model1.summary()

print(time_to_run)

plt.plot(history_fnn1.history['loss']) 
plt.plot(history_fnn1.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

# Neural network
fnn_model2 = Sequential()
fnn_model2.add(Dense(256, input_dim = 2304, activation = 'relu'))
fnn_model2.add(Dropout(0.5))
fnn_model2.add(Dense(64, activation= 'relu'))
fnn_model2.add(Dropout(0.2))
fnn_model2.add(Dense(7, activation = 'softmax'))

fnn_checkpoint2 = ModelCheckpoint("best_fnn_model2.hdf5", monitor = 'val_loss', verbose = 1, 
                                  save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002)

fnn_model2.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_fnn2 = fnn_model2.fit(X_train_fnn, y_train, 
                    validation_data = (X_val_fnn, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[fnn_checkpoint2])

time_to_run = time() - start

fnn_model2.summary()
print(time_to_run)

plt.plot(history_fnn2.history['loss']) 
plt.plot(history_fnn2.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

# Neural network
fnn_model3 = Sequential()
fnn_model3.add(Dense(512, input_dim = 2304, activation ='sigmoid'))
fnn_model3.add(Dense(64, activation = 'sigmoid'))
fnn_model3.add(Dropout(0.4))
fnn_model3.add(Dense(7, activation = 'softmax'))

fnn_checkpoint3 = ModelCheckpoint("best_fnn_model3.hdf5", monitor = 'val_loss', verbose = 1, 
                                  save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)

fnn_model3.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_fnn3 = fnn_model3.fit(X_train_fnn, y_train, 
                    validation_data = (X_val_fnn, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[fnn_checkpoint3])

time_to_run = time() - start

fnn_model3.summary()
print(time_to_run)

plt.plot(history_fnn3.history['loss']) 
plt.plot(history_fnn3.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

"""**c(i).** Details of the 3 different FNN models are listed below:


---



**#Details on FNN Model 1:**

Number of hidden layers: 2

Number of neurons in 4 layers: 128, 64, 32, 7

Total parameters: 305607

Optimizer used: Adam with learning rate of 0.001

Regulerizer used: l2 

Dropout: N/A

Activation function for input and hidden layers: ReLU

Activation function for output layer: Softmax

Training accuracy (for the best model): 43.35%

Validation accuracy (for the best model): 39.40%

Training time: 8.26 s


---


**#Details on FNN Model 2:**

Number of hidden layers: 1

Number of neurons in 3 layers: 256, 64, 7

Total parameters: 606983

Optimizer used: Adam with learning rate of 0.002

Regulerizer used: N/A 

Dropout: 0.5 after 1st layer, 0.2 after 2nd layer

Activation function for input and hidden layers: ReLU

Activation function for output layer: Softmax

Training accuracy (for the best model): 42.46%

Validation accuracy (for the best model): 41.07%

Training time: 10.95 s


---



**#Details on FNN Model 3:**

Number of hidden layers: 1

Number of neurons in 3 layers: 512, 64, 7

Total parameters: 1213447

Optimizer used: Adam with learning rate of 0.0005

Regulerizer used: N/A 

Dropout: 0.4 after 2nd layer

Activation function for input and hidden layers: Sigmoid

Activation function for output layer: Softmax

Training accuracy (for the best model): 53.48%

Validation accuracy (for the best model): 43.44%

Training time: 7.61 s

We get a very clear idea about how different models are performing with this training set from the listing above. Even though the model 3 achieves the best training and validation accuracy for the best weights saved during training process, it has an overfitting tendency as per the loss curve. On the other hand model 2 performed similarly on the validation set and has less overfitting tendency overall. So, model 2 can be safely chosen as the best model and used for testing.
"""

best_fnn_model = tf.keras.models.load_model("best_fnn_model3.hdf5")
score = best_fnn_model.evaluate(X_test_fnn, y_test, verbose=0)
print("%s: %.2f%%" % (best_fnn_model.metrics_names[1], score[1]*100))

"""**c(ii).** Emotion classification accuracy on the testing set = 43.69%"""

from keras.layers import Activation, Flatten, Conv2D, MaxPooling2D
from keras.layers import BatchNormalization
from keras.utils import np_utils

cnn_model1 = Sequential()

cnn_model1.add(Conv2D(filters=96, input_shape=(48, 48, 1), kernel_regularizer = regularizers.l2(l2=0.01), 
                      kernel_size=(11,11), strides=(4,4), padding='same'))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('relu'))
cnn_model1.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

cnn_model1.add(Conv2D(filters=256, kernel_size=(5, 5), kernel_regularizer = regularizers.l2(l2=0.01),
                      strides=(1,1), padding='same'))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('relu'))
cnn_model1.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

cnn_model1.add(Conv2D(filters=384, kernel_size=(3,3), kernel_regularizer = regularizers.l2(l2=0.01),
                      strides=(1,1), padding='same'))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('relu'))

cnn_model1.add(Flatten())

cnn_model1.add(Dense(4096, input_shape=(32,32,3,)))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('relu'))
cnn_model1.add(Dropout(0.4))

cnn_model1.add(Dense(1000))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('relu'))
cnn_model1.add(Dropout(0.4))

cnn_model1.add(Dense(7))
cnn_model1.add(BatchNormalization())
cnn_model1.add(Activation('softmax'))


cnn_model1.summary()

cnn_checkpoint1 = ModelCheckpoint("best_cnn_model1.hdf5", monitor = 'val_loss', verbose = 1,
    save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)

cnn_model1.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_cnn1 = cnn_model1.fit(X_train, y_train, 
                    validation_data = (X_val, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[cnn_checkpoint1])

time_to_run = time() - start

plt.plot(history_cnn1.history['loss']) 
plt.plot(history_cnn1.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

print(time_to_run)

cnn_model2 = Sequential()

cnn_model2.add(Conv2D(filters=96, input_shape=(48, 48, 1), kernel_regularizer = regularizers.l2(l2=0.01), 
                      kernel_size=(11,11), strides=(4,4), padding='same'))
cnn_model2.add(Activation('relu'))
cnn_model2.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

cnn_model2.add(Conv2D(filters=48, kernel_size=(3,3), kernel_regularizer = regularizers.l2(l2=0.01),
                      strides=(1,1), padding='same'))
cnn_model2.add(Activation('relu'))

cnn_model2.add(Flatten())

cnn_model2.add(Dense(1048, input_shape=(32,32,3,)))
cnn_model2.add(Activation('sigmoid'))
cnn_model2.add(Dropout(0.4))

cnn_model2.add(Dense(128))
cnn_model2.add(Activation('sigmoid'))
cnn_model2.add(Dropout(0.4))

cnn_model2.add(Dense(7))
cnn_model2.add(Activation('softmax'))


cnn_model2.summary()

cnn_checkpoint2 = ModelCheckpoint("best_cnn_model2.hdf5", monitor = 'val_loss', verbose = 1,
    save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)

cnn_model2.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_cnn2 = cnn_model2.fit(X_train, y_train, 
                    validation_data = (X_val, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[cnn_checkpoint2])

time_to_run = time() - start

plt.plot(history_cnn2.history['loss']) 
plt.plot(history_cnn2.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

print(time_to_run)

cnn_model3 = Sequential()

cnn_model3.add(Conv2D(filters=48, input_shape=(48, 48, 1), kernel_regularizer = regularizers.l2(l2=0.01), 
                      kernel_size=(5,5), strides=(2,2), padding='same'))
cnn_model3.add(Activation('relu'))
cnn_model3.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

cnn_model3.add(Conv2D(filters=16, kernel_size=(3,3), kernel_regularizer = regularizers.l2(l2=0.01),
                      strides=(1,1), padding='same'))
cnn_model3.add(Activation('relu'))
cnn_model3.add(Dropout(0.4))

cnn_model3.add(Flatten())

cnn_model3.add(Dense(256))
cnn_model3.add(Activation('relu'))
cnn_model3.add(Dropout(0.2))

cnn_model3.add(Dense(64))
cnn_model3.add(Activation('relu'))
cnn_model3.add(Dropout(0.2))

cnn_model3.add(Dense(7))
cnn_model3.add(Activation('softmax'))

cnn_model3.summary()

cnn_checkpoint3 = ModelCheckpoint("best_cnn_model3.hdf5", monitor = 'val_loss', verbose = 1,
    save_best_only = True, mode =' min', period = 1)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)

cnn_model3.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

start = time()

history_cnn3 = cnn_model3.fit(X_train, y_train, 
                    validation_data = (X_val, y_val), 
                    epochs = 20, batch_size = 512, 
                    validation_batch_size = 512, 
                    callbacks=[cnn_checkpoint3])

time_to_run = time() - start

plt.plot(history_cnn3.history['loss']) 
plt.plot(history_cnn3.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

print(time_to_run)

"""**d(i).** Details of the 3 different CNN models are listed below:


---



**#Details on CNN Model 1:**

Number of conv layers: 3

Number of pooling layers: 2

Total parameters: 19798723

Optimizer used: Adam with learning rate of 0.0005

Regulerizer used: l2 regulerizer used with conv layers 

Dropout: 0.4 after dense layers

Activation function for input and hidden layers: ReLU

Activation function for output layer: Softmax

Training accuracy (for the best model): 85.23%

Validation accuracy (for the best model): 46.45%

Training time: 83.49 s


---


**#Details on CNN Model 2:**

Number of conv layers: 2

Number of pooling layers: 1

Total parameters: 20003999

Optimizer used: Adam with learning rate of 0.0005

Regulerizer used: l2 regulerizer used with conv layers 

Dropout: 0.4 after dense layers

Activation function for input and hidden layers: ReLU for conv layers & Sigmoid for dense layers

Activation function for output layer: Softmax

Training accuracy (for the best model): 48.21%

Validation accuracy (for the best model): 47.34%

Training time: 12.48 s


---



**#Details on FNN Model 3:**

Number of conv layers: 2

Number of pooling layers: 1

Total parameters: 615159

Optimizer used: Adam with learning rate of 0.0005

Regulerizer used: l2 regulerizer used with conv layers 

Dropout: 0.4 after 2nd conv layers, 0.2 after dense layers

Activation function for input and hidden layers: ReLU

Activation function for output layer: Softmax

Training accuracy (for the best model): 56.25%

Validation accuracy (for the best model): 53.66%

Training time: 11.89 s

Analayzing the performance of the 3 models on train and validation sets, it can be concluded that the 3rd model performs well and we should use it for testing. One notable thing about the CNN models are they have higher number of parameters and runtime compared to the FNN model and their results are improved by roughly 10%.
"""

best_cnn_model = tf.keras.models.load_model("best_cnn_model3.hdf5")
score = best_cnn_model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (best_cnn_model.metrics_names[1], score[1]*100))

"""**d(ii).** Emotion classification accuracy on the testing set = 54.56%"""

!pip install -q -U keras-tuner

def build_model(hp):

    model = keras.Sequential([
 
    keras.layers.Conv2D(
        filters = hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),
        kernel_size = hp.Choice('conv_1_kernel', values = [3,5]),
        activation = hp.Choice('conv_1_activition', values = ['relu','sigmoid']),
        input_shape = (48,48,1),
        padding = 'same'),

    keras.layers.MaxPooling2D(
        pool_size = hp.Choice('pool_1_kernel', values = [2,4]), 
        strides = (2,2), 
        padding ='same'),

    keras.layers.Conv2D(
        filters = hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),
        kernel_size = hp.Choice('conv_2_kernel', values = [3,5]),
        activation = 'relu',
        padding = 'same'),

    keras.layers.MaxPooling2D(
        pool_size = hp.Choice('pool_2_kernel', values = [2,4]), 
        strides = (2,2), 
        padding = 'same'),
    
    keras.layers.Flatten(), 

    keras.layers.Dense(
        units = hp.Int('dense_1_units', min_value=64, max_value=512, step=16),
        activation = hp.Choice('dense_1_activition', values = ['relu','sigmoid'])),
  
    keras.layers.Dense(7, activation='softmax')
    ])


    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
  
    return model

#importing random search
from keras_tuner import BayesianOptimization
#creating randomsearch object
tuner = BayesianOptimization(build_model,
                    objective='val_loss',
                    max_trials = 5,
                    overwrite = True)
# search best parameter
tuner.search(X_train, y_train, epochs=10, batch_size = 512, validation_data=(X_val, y_val))

model = tuner.get_best_models(num_models=1)[0]
print (model.summary())
# Evaluate the best model.
loss, accuracy = model.evaluate(X_test, y_test)
print('loss:', loss)
print('accuracy:', accuracy*100)

"""**e.** Emotion classification accuracy on the testing set = 55.73%"""

from tensorflow.keras.applications import VGG16
vgg_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (48, 48, 3))

# Freeze bottom layers
for layer in vgg_model.layers[:17]:
    layer.trainable = False

for i, layer in enumerate(vgg_model.layers):
    print(i, layer.name, layer.trainable)

x = vgg_model.output
x = Flatten()(x) 
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x) 
x = Dense(7, activation='softmax')(x)

transfer_model = tf.keras.Model(inputs = vgg_model.input, outputs = x)

checkpoint3 = ModelCheckpoint('vgg16_finetune.h15', 
                              monitor= 'val_loss', 
                              mode= 'min', 
                              save_best_only = True, 
                              verbose= 1)

from tensorflow.keras.optimizers import Adam
learning_rate= 0.005
transfer_model.compile(loss="categorical_crossentropy", optimizer=Adam(lr=learning_rate), metrics=["accuracy"])

X_train_trn = np.stack((X_train, X_train, X_train), axis = 3)
X_test_trn = np.stack((X_test, X_test, X_test), axis = 3)
X_val_trn = np.stack((X_val, X_val, X_val), axis = 3)

transfer_history = transfer_model.fit(
    X_train_trn, y_train,
    batch_size = 512,
    epochs=20,
    validation_data=(X_val_trn, y_val),
    callbacks = [checkpoint3])

plt.plot(transfer_history.history['loss']) 
plt.plot(transfer_history.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

best_trn_model = tf.keras.models.load_model("vgg16_finetune.h15")
score = best_trn_model.evaluate(X_test_trn, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

"""**(f).** I have used VGG-16 pretrained model and fine-tuned it with the training data. I started with pretrained imagenet weights for training and froze all the conv blocks except the last conv and pool layers and fine tuned the weights for those layers and the dense layers at the top. The accuracy of the best model is 48.59% on the test set."""

X_train_aug = np.expand_dims(X_train, axis=-1)
X_val_aug = np.expand_dims(X_val, axis=-1)
X_test_aug = np.expand_dims(X_test, axis=-1)

from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
    fill_mode='nearest',
    rotation_range=20,
    shear_range=20,
    zoom_range=0.1,
    horizontal_flip = True)

valid_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(
    X_train_aug, y_train,
    batch_size = 512)
    
validation_generator = valid_datagen.flow(
    X_val_aug, y_val,
    batch_size = 256)

n_rows=3
n_cols=5
n_images = n_rows * n_cols

plt.figure(figsize=(n_cols*4, n_rows*3))
for image_index in range(n_images):
    image, label = next(train_generator)
    img = image[0].squeeze()
    plt.subplot(n_rows, n_cols, image_index+1)
    plt.axis('off')
    plt.imshow(img, cmap = 'gray')

checkpoint_4 = ModelCheckpoint("best_cnn_aug_model.hdf5", monitor = 'val_loss', verbose = 1,
    save_best_only = True, mode =' min', period = 1)

history = model.fit_generator(train_generator, 
                              validation_data=validation_generator,
                              epochs = 10,
                              callbacks=[checkpoint_4])

plt.plot(history.history['loss']) 
plt.plot(history.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()

model = tf.keras.models.load_model("best_cnn_aug_model.hdf5")
score = model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

"""**g.** I have augmented the model using image data generator. For data augmentation, I have used rotation, shear, zoom and horizontal flip. Some of the augmented images are also displayed in a cell above. I have also trained a cnn model (model obtained from previous Bayesian optimization step) using augmented data. The accuracy on the test set is 58.26%."""